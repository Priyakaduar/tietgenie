import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_together import ChatTogether

print("ğŸ”§ Script is running...")

# âœ… Step 1: Check API key
api_key = os.getenv("TOGETHER_API_KEY")
if not api_key:
    raise ValueError("ğŸš« TOGETHER_API_KEY is not set!")
print("ğŸ” API key detected.")

# âœ… Step 2: Load PDF
pdf_path = "rules.pdf"  # ğŸ” Change this to your actual PDF name if needed
if not os.path.exists(pdf_path):
    raise FileNotFoundError(f"âŒ PDF not found at path: {pdf_path}")

loader = PyPDFLoader(pdf_path)
documents = loader.load()
print(f"ğŸ“„ Loaded {len(documents)} page(s) from PDF.")

# âœ… Step 3: Text splitting
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = splitter.split_documents(documents)
print(f"âœ‚ï¸ Split into {len(split_docs)} chunks.")

# âœ… Step 4: Embeddings + FAISS vector store
print("ğŸ§  Embedding and indexing...")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(split_docs, embeddings)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})

# âœ… Step 5: Load Together LLM
print("ğŸ¤– Loading Together LLM...")
llm = ChatTogether(model="deepseek-ai/DeepSeek-V3", temperature=0.2)

# âœ… Step 6: Create QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# âœ… Step 7: Ask questions
while True:
    query = input("\nâ“ Ask a question (or type 'exit' to quit): ")
    if query.lower() == "exit":
        print("ğŸ‘‹ Exiting.")
        break

    print("ğŸ’¬ Thinking...")
    try:
        result = qa_chain(query)
        clean_answer = result["result"].replace("**", "")
        print("\nâœ… Answer:\n", clean_answer)


        print("\nğŸ“š Source snippet(s):")
        for i, doc in enumerate(result["source_documents"]):
            print(f"- Snippet {i+1}:", doc.page_content[:300], "...\n")
    except Exception as e:
        print("âŒ Error during QA:", e)

